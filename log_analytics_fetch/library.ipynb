{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "from delta.tables import *\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting keyvault name stored in spark configuration\n",
    "keyvault_name = spark.conf.get('key_vault_name')\n",
    "storage_account_name = spark.conf.get('storage_account_name')\n",
    "\n",
    "# we can create the databricks secretScope with same name as our Azure Keyvault to which it is connected\n",
    "tenant_id = dbutils.secret.get(keyvault_name,'tennt_id')\n",
    "\n",
    "#set spark configuration for Azure Storage account connection using service principal\n",
    "spark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\") \n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id\", dbutils.secrets.get(keyvault_name,'adls-client-id')) \n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret\", dbutils.secrets.get(keyvault_name,'adls-client-secret')) \n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import ClientSecretCredential\n",
    "from azure.monitor.query import LogsQueryClient,LogsQueryStatus\n",
    "\n",
    "class sp_cred:\n",
    "    def __init__(self,tenant_id,client_id,client_secret):\n",
    "        self.tenant_id = tenant_id\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "    def get_credential(self):\n",
    "        credential = ClientSecretCredential(\n",
    "            tenant_id=self.tenant_id,\n",
    "            client_id = self.client_id,\n",
    "            client_secret = self.client_secret\n",
    "        )\n",
    "\n",
    "        return credential\n",
    "\n",
    "class Log_fetch(sp_cred):\n",
    "    def __init__(self, tenant_id, client_id, client_secret,workspace_id,timespan):\n",
    "        super().__init__(tenant_id, client_id, client_secret)\n",
    "        self.timespan=timespan\n",
    "        self.workspace_id = workspace_id\n",
    "\n",
    "    def fetch_logs(self,query_):\n",
    "        cred = super().get_credential()\n",
    "        log_analytics_client = LogsQueryClient(cred)\n",
    "        response = log_analytics_client.query_workspace(\n",
    "            workspace_id = self.workspace_id,\n",
    "            query = query_,\n",
    "            timespan = self.timespan\n",
    "        )\n",
    "        if response.status == LogsQueryStatus.SUCCESS:\n",
    "            data = response.tables\n",
    "            return data\n",
    "        else:\n",
    "            return 'Response Fail'\n",
    "        \n",
    "    def fetch_log_gt_xuid_cnt(self,query_base):\n",
    "        query_ =  query_base + \"\"\"| count\"\"\"\n",
    "        response_data = self.fetch_logs(query_)\n",
    "        if response_data == 'Response Fail':\n",
    "            return 'Response Failed, Please check and try again'\n",
    "        else:\n",
    "            for table in response_data:\n",
    "                iteration_count = table.rows[0][0]\n",
    "        return iteration_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting max read id from the watermark table - for incrementally loading the data \n",
    "# Here maintaining watermark table in delta table\n",
    "\n",
    "def get_max_uid(log_name,tab_name,env_name):\n",
    " if not(spark._jsparkSession.catalog().tableExists(tab_name)):\n",
    "    max_last_uid = '0'\n",
    " else:\n",
    "    max_last_uid = spark.sql(f\"\"\"select max(uid_max) as max_uid from {tab_name} where pipeline_name='{log_name}' and env='{env_name}'\"\"\").collect()[0][0]\n",
    "    if max_last_uid is None:\n",
    "        max_last_uid = '0'\n",
    "\n",
    " return max_last_uid\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#UPSERT\n",
    "def merge_into(max_uid_last,tab_name,env_name):\n",
    "    if not(spark._jsparkSession.catalog().tableExists(tab_name)):\n",
    "        query_ = f'''create table {tab_name}\n",
    "        ( uid_max string,\n",
    "        pipeline_name string,\n",
    "        env string)\n",
    "        using DELTA; '''\n",
    "        # if not(spark._jsparkSession.catalog().tableExists(tab_name)):\n",
    "        # query_ = f'''create table {tab_name}\n",
    "        # ( uid_max string,\n",
    "        # pipeline_name string,\n",
    "        # env string)\n",
    "        # using DELTA; '''\n",
    "        spark.sql(query_)\n",
    "    sink_table = DeltaTable.forName(spark, tab_name)\n",
    "    (sink_table.alias('source').merge(\n",
    "    max_uid_last.alias('updates'),'source.pipeline_name = updates.pipeline_name and source.env=updates.env'\n",
    "    ).\n",
    "    whenMatchedUpdate(set =\n",
    "    {'uid_max':\"updates.uid_max\",\n",
    "    'pipeline_name':\"updates.pipeline_name\",\n",
    "    'env':\"updates.env\"})\n",
    "    .whenNotMatchedInsert(values =\n",
    "    {'uid_max':\"updates.uid_max\",\n",
    "    'pipeline_name':\"updates.pipeline_name\",\n",
    "    'env':\"updates.env\"})\n",
    "    ).execute()\n",
    "    print('Rows has been updated.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
